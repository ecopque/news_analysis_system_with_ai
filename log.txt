# FILE: /log.txt

# [Step by step]:

#1: $pip install requests beautifulsoup4 feedparser pandas python-dotenv streamlit ;
#2: $pip install google-generativeai ;

#3: requests: Used to send HTTP requests to websites and retrieve their content.
#4: BeautifulSoup: Parses HTML content from web pages for data extraction.
#5: feedparser: Parses RSS feeds to extract news articles.
#6: pandas: Handles data processing and structuring using DataFrames.
#7: datetime: Manages date and time formatting.
#8: time: Adds delays in execution (useful for preventing excessive requests).
#9: urljoin: Combines relative URLs with base URLs to create absolute links.

#10: Defines the NewsCollector class, responsible for fetching news from websites and RSS feeds.
#11: 'User-Agent': 'Mozilla/5.0' makes the request look like it’s coming from a real web browser.

#12: This method fetches HTML content from a given url, extracts news articles, and returns them as a DataFrame.
#13: Sends an HTTP GET request to the url. Includes self.headers, making the request appear to be from a browser. Sets a timeout=10, ensuring it waits a maximum of 10 seconds for a response.
#14: response.text extracts the raw HTML content from the page. Converts the HTML string into a structured object. 'html.parser' is a built-in parser in Python that helps navigate and extract elements easily.
#15: Finds all <article> elements in the HTML. Limits extraction to 10 articles.
'''
HTML example for "i1":

<html>
  <body>
    <article>
      <h2>Breaking News</h2>
      <a href="/news1">Read more</a>
    </article>
  </body>
</html>
'''
#16: Searches for <h2> inside the <article>. If <h2> exists, extracts its text and removes extra spaces using .strip(). If not found, assigns "Untitled" as the title.
#17: Looks for an <a> (anchor/link tag) inside the <article>. If found, extracts its href (URL). Uses urljoin(url, link) to convert relative URLs into absolute URLs.
'''
# Base URL
url = "https://newswebsite.com"
# Found <a> tag
href = "/news1"
# urljoin combines them:
full_url = urljoin(url, href)
# Result: "https://newswebsite.com/news1"
'''
#18: Stores the extracted data in a dictionary. Adds the dictionary to the articles list.
#19: If an error occurs, prints a message and returns an empty DataFrame.

#20: Fetches news articles from an RSS feed.
#21: Parses the RSS feed using feedparser.parse(). Creates an empty list to store extracted articles.
#22: Iterates through RSS feed entries. Limits extraction to 15 articles.
#23: Extracts title, URL, source, and date from each entry. Appends the data to the list.

#24: Loops through a list of sources (websites and RSS feeds).
#25: Creates an empty DataFrame to store all articles.
#26: Checks if the source is an RSS feed (.xml) or a website. Calls the correct function (get_rss or scrape_site).
#27: Merges the new articles into the existing DataFrame.
#28: Waits 2 seconds before fetching the next source.
#29: Ensures no duplicate articles exist in the final DataFrame.

#30: Used to make HTTP requests to AI APIs.
#31: Loads environment variables from a .env file.
#32: Loads variables defined in the .env file.

#33: Retrieves the API key from the environment variables. If the key is not set, self.api_key will be None.
#34: Defines the API endpoint, which is the URL where requests will be sent.
#35: This part of the code is used to create HTTP headers for an API request. Headers are additional pieces of information sent with an HTTP request. They help define how the request should be processed by the server. "Authorization": f"Bearer {self.api_key}" → Adds the API key in the request header. "Content-Type": "application/json" → Specifies that the request will be in JSON format.
#36: 'model': 'deepseek-chat' → Defines which DeepSeek model will be used. 'messages' → Sends the message to the AI in the required format. The role (role) of the user is "user", indicating that the user is starting the conversation. The content (content) includes the text to be analyzed. 'temperature': temperature → Controls the creativity of the AI’s response (higher values = more creative responses). 'max_tokens': 1000 → Limits the response to 1000 tokens.

#37: requests.post(self.base_url, headers=headers, json=payload) → Makes a POST request to the API, sending the data in the request body. The JSON format is ensured by the json=payload parameter.
#38: response.raise_for_status() → If the response indicates an error (e.g., 400, 500), an exception is raised.

#39: response.json() → Converts the API response into a Python dictionary.
#40: If 'choices' is not present, returns an error message.
#41: If everything is correct, returns the AI’s response content.

#42: Configures the Google Gemini API with the API key stored in environment variables.
#43: Defines the Gemini 1.5 Flash model, optimized for fast responses.
#44: Initially, no chat is active.
#45: Stores a predefined context that guides how the AI should analyze news data.

#46: Initiates a new chat session with Gemini AI.
#47: Sends an initial message to provide the AI with the context and news data.

#48: If no chat is active (self.chat is None), returns an error message.
#49: Sends the question to Gemini AI and returns the response.