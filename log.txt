# FILE: /log.txt

# [Step by step]:
#1: $pip install requests beautifulsoup4 feedparser pandas python-dotenv streamlit ;
#2: $pip install google-generativeai ;

#3: requests: Used to send HTTP requests to websites and retrieve their content.
#4: BeautifulSoup: Parses HTML content from web pages for data extraction.
#5: feedparser: Parses RSS feeds to extract news articles.
#6: pandas: Handles data processing and structuring using DataFrames.
#7: datetime: Manages date and time formatting.
#8: time: Adds delays in execution (useful for preventing excessive requests).
#9: urljoin: Combines relative URLs with base URLs to create absolute links.

#10: Defines the NewsCollector class, responsible for fetching news from websites and RSS feeds.
#11: 'User-Agent': 'Mozilla/5.0' makes the request look like itâ€™s coming from a real web browser.

#12: This method fetches HTML content from a given url, extracts news articles, and returns them as a DataFrame.
#13: Sends an HTTP GET request to the url. Includes self.headers, making the request appear to be from a browser. Sets a timeout=10, ensuring it waits a maximum of 10 seconds for a response.
#14: response.text extracts the raw HTML content from the page. Converts the HTML string into a structured object. 'html.parser' is a built-in parser in Python that helps navigate and extract elements easily.

#15: Finds all <article> elements in the HTML. Limits extraction to 10 articles.

#16: Searches for <h2> inside the <article>. If <h2> exists, extracts its text and removes extra spaces using .strip(). If not found, assigns "Untitled" as the title.

#17: Looks for an <a> (anchor/link tag) inside the <article>. If found, extracts its href (URL). Uses urljoin(url, link) to convert relative URLs into absolute URLs.