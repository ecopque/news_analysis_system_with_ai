# FILE: /log.txt

# [Step by step]:
#1: $pip install requests beautifulsoup4 feedparser pandas python-dotenv streamlit ;
#2: $pip install google-generativeai ;

#3: requests: Used to send HTTP requests to websites and retrieve their content.
#4: BeautifulSoup: Parses HTML content from web pages for data extraction.
#5: feedparser: Parses RSS feeds to extract news articles.
#6: pandas: Handles data processing and structuring using DataFrames.
#7: datetime: Manages date and time formatting.
#8: time: Adds delays in execution (useful for preventing excessive requests).
#9: urljoin: Combines relative URLs with base URLs to create absolute links.

#10: Defines the NewsCollector class, responsible for fetching news from websites and RSS feeds.
#11: 'User-Agent': 'Mozilla/5.0' makes the request look like itâ€™s coming from a real web browser.

#12: This method fetches HTML content from a given url, extracts news articles, and returns them as a DataFrame.
#13: Sends an HTTP GET request to the url. Includes self.headers, making the request appear to be from a browser. Sets a timeout=10, ensuring it waits a maximum of 10 seconds for a response.
#14: response.text extracts the raw HTML content from the page. Converts the HTML string into a structured object. 'html.parser' is a built-in parser in Python that helps navigate and extract elements easily.

#15: Finds all <article> elements in the HTML. Limits extraction to 10 articles.
'''
HTML example for "i1":

<html>
  <body>
    <article>
      <h2>Breaking News</h2>
      <a href="/news1">Read more</a>
    </article>
  </body>
</html>
'''

#16: Searches for <h2> inside the <article>. If <h2> exists, extracts its text and removes extra spaces using .strip(). If not found, assigns "Untitled" as the title.

#17: Looks for an <a> (anchor/link tag) inside the <article>. If found, extracts its href (URL). Uses urljoin(url, link) to convert relative URLs into absolute URLs.
'''
# Base URL
url = "https://newswebsite.com"

# Found <a> tag
href = "/news1"

# urljoin combines them:
full_url = urljoin(url, href)
# Result: "https://newswebsite.com/news1"
'''

#18: Stores the extracted data in a dictionary. Adds the dictionary to the articles list.

#19: If an error occurs, prints a message and returns an empty DataFrame.

#20: Fetches news articles from an RSS feed.
#21: Parses the RSS feed using feedparser.parse(). Creates an empty list to store extracted articles.

#22: Iterates through RSS feed entries. Limits extraction to 15 articles.